<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Smile Detector</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        body { text-align: center; font-family: Arial, sans-serif; }
        #video { display: block; margin: 0 auto; }
        #canvas { position: absolute; top: 0; left: 0; }
        #message { font-size: 24px; color: green; transition: all 0.3s ease; }
        #counter { font-size: 20px; color: blue; }
        #captureButton { margin-top: 20px; padding: 10px 20px; font-size: 16px; background-color: #4CAF50; color: white; border: none; cursor: pointer; }
        #toggleButton { margin-top: 20px; padding: 10px 20px; font-size: 16px; background-color: #FF5733; color: white; border: none; cursor: pointer; }
    </style>
</head>
<body>
    <div class="face">
    <h1>Smile Detector</h1>
    <video id="video" width="640" height="480" autoplay></video>
    <canvas id="canvas"></canvas>
    <div id="message"></div>
    <div id="counter">Smile count: 0</div>
    <button id="captureButton" onclick="captureSmile()">Capture Smile</button>
    <button id="toggleButton" onclick="toggleCamera()">Toggle Camera</button>
</div>
    <script>
        let smileCount = 0; 
        let videoStream = null; 
        let videoElement = document.getElementById('video'); 

        window.onload = async function() {
            if (typeof faceapi !== 'undefined') {
                console.log("face-api.js loaded correctly");
                await loadModels();
            } else {
                console.error('faceapi is not defined.');
            }
        };

        // Load models for face and expression detection
        async function loadModels() {
            try {
                console.log("Loading models...");
                await faceapi.nets.ssdMobilenetv1.loadFromUri('/models');
                await faceapi.nets.faceExpressionNet.loadFromUri('/models');
                console.log("Models loaded successfully.");
                startDetection();
            } catch (error) {
                console.error("Error loading models:", error);
            }
        }

        // Start video stream and detection
        async function startDetection() {
            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            videoElement.srcObject = stream;
            videoStream = stream; // Save stream to toggle later

            videoElement.onplay = () => {
                console.log("Video is playing.");
                
                const canvas = faceapi.createCanvasFromMedia(videoElement);
                document.body.append(canvas);
                const displaySize = { width: videoElement.width, height: videoElement.height };
                faceapi.matchDimensions(canvas, displaySize);

                setInterval(async () => {
                    try {
                        const detections = await faceapi.detectAllFaces(videoElement).withFaceExpressions();
                        const context = canvas.getContext('2d');
                        if (context) {
                            context.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
                        }

                        faceapi.draw.drawDetections(canvas, detections);
                        faceapi.draw.drawFaceExpressions(canvas, detections);

                        const expressions = detections[0]?.expressions;
                        if (expressions) {
                            // Find the expression with the highest value
                            const highestExpression = Object.entries(expressions).reduce((max, entry) => {
                                return entry[1] > max[1] ? entry : max;
                            });

                            // Display the detected expression
                            const [expression, value] = highestExpression;
                            document.getElementById('message').textContent = `Expression: ${expression.charAt(0).toUpperCase() + expression.slice(1)} (${Math.round(value * 100)}%)`;

                            // If the user is smiling, increase the smile count
                            if (expressions.happy > 0.5) {
                                smileCount++;
                                document.getElementById('counter').textContent = `Smile count: ${smileCount}`;
                            }
                        }
                    } catch (error) {
                        console.error("Error during face detection:", error);
                    }
                }, 100);
            };
        }

        // Capture the smile and save it as an image
        function captureSmile() {
            const canvas = faceapi.createCanvasFromMedia(videoElement);
            document.body.append(canvas);
            const context = canvas.getContext('2d');
            context.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
            
            // Create a link to download the image
            const image = canvas.toDataURL('image/png');
            const link = document.createElement('a');
            link.href = image;
            link.download = 'smile.png';
            link.click();
        }

        // Toggle camera on and off
        function toggleCamera() {
            if (videoStream) {
                // Stop all tracks in the stream to turn off the camera
                videoStream.getTracks().forEach(track => track.stop());
                videoElement.srcObject = null;
                document.getElementById('toggleButton').textContent = "Start Camera";
            } else {
                startDetection(); // Restart the camera if it's turned off
                document.getElementById('toggleButton').textContent = "Stop Camera";
            }
        }
    </script>
</body>
</html>
